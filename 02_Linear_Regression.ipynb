{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinaremos nosso modelo ajustando os pesos várias vezes para fazer previsões cada vez melhores, utilizando uma técnica de otimização chamada gradiente de descida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos representar os dados de treinamento utilizando duas matrizes: `inputs` e `targets`, cada uma com uma linha por observação e uma coluna por variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temperatura, chuva, umidade)\n",
    "inputs = np.array(\n",
    "    [[73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70]],\n",
    "    dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets (maçãs, laranjas)\n",
    "targets = np.array(\n",
    "    [[56, 70], [81, 101], [119, 133], [22, 37], [103, 119]], dtype=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 73.  67.  43.]\n",
      " [ 91.  88.  64.]\n",
      " [ 87. 134.  58.]\n",
      " [102.  43.  37.]\n",
      " [ 69.  96.  70.]]\n",
      "[[ 56.  70.]\n",
      " [ 81. 101.]\n",
      " [119. 133.]\n",
      " [ 22.  37.]\n",
      " [103. 119.]]\n"
     ]
    }
   ],
   "source": [
    "# Converte os inputs e targets para tensores\n",
    "inputs_t = torch.from_numpy(inputs)\n",
    "targets_t = torch.from_numpy(targets)\n",
    "\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os pesos e os viéses também podem ser representados por matrizes, inicializados como valores aleatórios por `randn`. A primeira linha de `w` e o primeiro elemento de `b` são usados para predizer a primeira variável-alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.6039e-01, -1.1863e-03,  6.3681e-01],\n",
      "        [-1.3369e+00,  2.6440e-01,  2.2328e-01]], requires_grad=True)\n",
      "tensor([-1.4243,  0.9292], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Pesos e bias\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -44.2298,  -69.3488],\n",
       "        [ -48.1689,  -83.1718],\n",
       "        [ -48.2027,  -67.0013],\n",
       "        [ -75.8736, -115.8043],\n",
       "        [ -23.2288,  -50.3051]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matriz obtida pela operação `x @ w.t() + b` é a predição do modelo para as variáveis alvos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -44.2298,  -69.3488],\n",
      "        [ -48.1689,  -83.1718],\n",
      "        [ -48.2027,  -67.0013],\n",
      "        [ -75.8736, -115.8043],\n",
      "        [ -23.2288,  -50.3051]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Gerando previsões\n",
    "preds = model(inputs_t)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# Comparando com os targets\n",
    "print(targets_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de perda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos um modelo, podemos melhorá-lo.\n",
    "\n",
    "Precisamos, antes desse passo, encontrar uma forma de saber se a predição foi além ou de menos. Podemos comparar o alvo com a previsão realizada com os seguintes meios:\n",
    "\n",
    "* Calculando a diferença entre as duas matrizes\n",
    "* Elevar todos os elementos ao quadrado para remover os valores negativos\n",
    "* Calcular a média dos elementos resultantes\n",
    "\n",
    "O resultado é um único valor, conhecido como MSE (mean square error) -- Erro Quadrático Médio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erro quadrático médio\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22555.1523, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculando o erro\n",
    "loss = mse(preds, targets_t)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradiente de loss: None\n",
      "Gradiente de w: tensor([[-10462.1367, -11362.7910,  -6946.3579],\n",
      "        [-14320.0586, -15033.5059,  -9376.8359]])\n"
     ]
    }
   ],
   "source": [
    "# Gradientes\n",
    "print(f'Gradiente de loss: {loss.backward()}')\n",
    "print(f'Gradiente de w: {w.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustando pesos e bias para reduzir a perda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo de todo treinamento de modelo é diminuir a perda.\n",
    "\n",
    "Se o grandiente é positivo: \n",
    "\n",
    "* aumentar o valor do peso, aumentará o valor da perda.\n",
    "* diminuir o valor do peso, diminuirá o valor da perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15218.1172, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Verificamos se as perdas diminuiram\n",
    "preds = model(inputs_t)\n",
    "loss = mse(preds, targets_t)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento utilizando descida do gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como visto, reduzimos a perda para melhorar o modelo usando algoritmos de descida de gradiente. Para treinar modelos seguindo essa estratégia, precisamos:\n",
    "\n",
    "1. Gerar predições\n",
    "2. Calcular a perda\n",
    "3. Calcular os gradientes\n",
    "4. Ajustar os pesos substraindo por uma pequena fração do gradiente\n",
    "5. Redefinir os gradientes a zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-25.9912, -44.7890],\n",
      "        [-24.2022, -50.9082],\n",
      "        [-19.8444, -28.9577],\n",
      "        [-57.7449, -91.2623],\n",
      "        [ -0.2380, -19.4266]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Geramos predições\n",
    "preds = model(inputs_t)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15218.1172, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculamos a perda\n",
    "loss = mse(preds, targets_t)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -8579.1221,  -9339.2451,  -5697.7505],\n",
      "        [-11785.5498, -12313.0684,  -7697.4321]])\n",
      "tensor([-101.8041, -139.0687])\n"
     ]
    }
   ],
   "source": [
    "# Computamos os gradientes\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos o peso\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7700,  0.2058,  0.7632],\n",
      "        [-1.0758,  0.5379,  0.3940]], requires_grad=True)\n",
      "tensor([-1.4220,  0.9323], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10273.5918, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculamos a perda\n",
    "preds = model(inputs_t)\n",
    "loss = mse(preds, targets_t)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento por épocas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para reduzirmos ainda mais a perda, podemos repetir esse processo por várias vezes (épocas). Vamos treinar por 100 épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento por 100 epochs\n",
    "for i in range(800):\n",
    "    preds = model(inputs_t)\n",
    "    loss = mse(preds, targets_t)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4225, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculamos a perda\n",
    "preds = model(inputs_t)\n",
    "loss = mse(preds, targets_t)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 56.7901,  70.5273],\n",
       "         [ 83.5668,  99.8954],\n",
       "         [116.1986, 134.3726],\n",
       "         [ 20.4441,  37.2765],\n",
       "         [104.3783, 117.8230]], grad_fn=<AddBackward0>),\n",
       " array([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]], dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificamos as predições com os targets\n",
    "preds, targets\n",
    "\n",
    "# Registrando o loss de 8.6:\n",
    "# (tensor([[ 57.3090,  69.2779],\n",
    "#          [ 82.1458,  99.8025],\n",
    "#          [118.6023, 136.5943],\n",
    "#          [ 21.4513,  29.5221],\n",
    "#          [101.6896, 122.3019]], grad_fn=<AddBackward0>),\n",
    "#  array([[ 56.,  70.],\n",
    "#         [ 81., 101.],\n",
    "#         [119., 133.],\n",
    "#         [ 22.,  37.],\n",
    "#         [103., 119.]], dtype=float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão linear utilizando ferramentas do PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs (temperatura, chuva, umidade)\n",
    "inputs = np.array(\n",
    "    [\n",
    "        [73, 67, 43],\n",
    "        [91, 88, 64],\n",
    "        [87, 134, 58],\n",
    "        [102, 43, 37],\n",
    "        [69, 96, 70],\n",
    "        [74, 66, 43],\n",
    "        [91, 87, 65],\n",
    "        [88, 134, 59],\n",
    "        [101, 44, 37],\n",
    "        [68, 96, 71],\n",
    "        [73, 66, 44],\n",
    "        [92, 87, 64],\n",
    "        [87, 135, 57],\n",
    "        [103, 43, 36],\n",
    "        [68, 97, 70],\n",
    "    ],\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "# Targets (maçãs, laranjas)\n",
    "targets = np.array(\n",
    "    [\n",
    "        [56, 70],\n",
    "        [81, 101],\n",
    "        [119, 133],\n",
    "        [22, 37],\n",
    "        [103, 119],\n",
    "        [57, 69],\n",
    "        [80, 102],\n",
    "        [118, 132],\n",
    "        [21, 38],\n",
    "        [104, 118],\n",
    "        [57, 69],\n",
    "        [82, 100],\n",
    "        [118, 134],\n",
    "        [20, 38],\n",
    "        [102, 120],\n",
    "    ],\n",
    "    dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.],\n",
       "        [ 74.,  66.,  43.],\n",
       "        [ 91.,  87.,  65.],\n",
       "        [ 88., 134.,  59.],\n",
       "        [101.,  44.,  37.],\n",
       "        [ 68.,  96.,  71.],\n",
       "        [ 73.,  66.,  44.],\n",
       "        [ 92.,  87.,  64.],\n",
       "        [ 87., 135.,  57.],\n",
       "        [103.,  43.,  36.],\n",
       "        [ 68.,  97.,  70.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos o dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos o DataLoader resposável por gerar os batches\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Shuffle` ajuda a randomizar o lote antes de passar para o modelo. Ele ajuda a melhorar a eficiência do algoritmo de otimização e leva a uma melhora acelerada da perda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 91.,  88.,  64.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [103.,  43.,  36.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 68.,  97.,  70.]])\n",
      "tensor([[ 81., 101.],\n",
      "        [ 56.,  70.],\n",
      "        [ 20.,  38.],\n",
      "        [ 22.,  37.],\n",
      "        [102., 120.]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3330,  0.4656, -0.3036],\n",
      "        [ 0.2406,  0.0802, -0.1553]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3518, -0.0056], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Definimos o modelo\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método `.parameters` retorna uma lista das matrizes dos pesos e do bias no modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.3330,  0.4656, -0.3036],\n",
       "         [ 0.2406,  0.0802, -0.1553]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.3518, -0.0056], requires_grad=True)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parametros\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42.7986, 16.2518],\n",
       "        [52.1935, 19.0049],\n",
       "        [74.1005, 22.6630],\n",
       "        [43.1032, 22.2363],\n",
       "        [46.7704, 13.4215],\n",
       "        [42.6660, 16.4122],\n",
       "        [51.4243, 18.7694],\n",
       "        [74.1298, 22.7483],\n",
       "        [43.2358, 22.0759],\n",
       "        [46.1338, 13.0256],\n",
       "        [42.0294, 16.0163],\n",
       "        [52.0609, 19.1653],\n",
       "        [74.8697, 22.8985],\n",
       "        [43.7399, 22.6322],\n",
       "        [46.9030, 13.2611]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Geramos as predições\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos a função de perda\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3978.8047, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar otimizadores embutidos no PyTorch em vez de definir manualmente. Para o gradiente de descida, podemos utilizar `optim.SGD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos o otimizador\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos a função de treinamento\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            # Geramos as predições\n",
    "            pred = model(xb)\n",
    "            # Calculamos a perda\n",
    "            loss = loss_fn(pred, yb)\n",
    "            # Computamos os gradientes\n",
    "            loss.backward()\n",
    "            # Atualizamos os parâmetros usando os gradientes\n",
    "            opt.step()\n",
    "            # Zeramos os gradientes\n",
    "            opt.zero_grad()\n",
    "        # Imprimimos a perda a cada 10 epocas\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\n",
    "                \"Epoch [{}/{}], Loss: {:.4f}\".format(epoch + 1, num_epochs, loss.item())\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/980], Loss: 572.2422\n",
      "Epoch [20/980], Loss: 241.2220\n",
      "Epoch [30/980], Loss: 12.5623\n",
      "Epoch [40/980], Loss: 243.3409\n",
      "Epoch [50/980], Loss: 66.4738\n",
      "Epoch [60/980], Loss: 108.7296\n",
      "Epoch [70/980], Loss: 46.1147\n",
      "Epoch [80/980], Loss: 99.3079\n",
      "Epoch [90/980], Loss: 28.0828\n",
      "Epoch [100/980], Loss: 67.3047\n",
      "Epoch [110/980], Loss: 38.7118\n",
      "Epoch [120/980], Loss: 39.8659\n",
      "Epoch [130/980], Loss: 22.8745\n",
      "Epoch [140/980], Loss: 43.4749\n",
      "Epoch [150/980], Loss: 28.8155\n",
      "Epoch [160/980], Loss: 39.9774\n",
      "Epoch [170/980], Loss: 33.0573\n",
      "Epoch [180/980], Loss: 20.0737\n",
      "Epoch [190/980], Loss: 33.1086\n",
      "Epoch [200/980], Loss: 8.5484\n",
      "Epoch [210/980], Loss: 13.6824\n",
      "Epoch [220/980], Loss: 20.2416\n",
      "Epoch [230/980], Loss: 18.3254\n",
      "Epoch [240/980], Loss: 7.8598\n",
      "Epoch [250/980], Loss: 23.4201\n",
      "Epoch [260/980], Loss: 14.4729\n",
      "Epoch [270/980], Loss: 10.9712\n",
      "Epoch [280/980], Loss: 16.3766\n",
      "Epoch [290/980], Loss: 12.1737\n",
      "Epoch [300/980], Loss: 14.4727\n",
      "Epoch [310/980], Loss: 12.1568\n",
      "Epoch [320/980], Loss: 12.2224\n",
      "Epoch [330/980], Loss: 16.5776\n",
      "Epoch [340/980], Loss: 12.3233\n",
      "Epoch [350/980], Loss: 9.1251\n",
      "Epoch [360/980], Loss: 16.3745\n",
      "Epoch [370/980], Loss: 7.2939\n",
      "Epoch [380/980], Loss: 10.5243\n",
      "Epoch [390/980], Loss: 6.8686\n",
      "Epoch [400/980], Loss: 7.0586\n",
      "Epoch [410/980], Loss: 10.5451\n",
      "Epoch [420/980], Loss: 10.2708\n",
      "Epoch [430/980], Loss: 8.0579\n",
      "Epoch [440/980], Loss: 6.7118\n",
      "Epoch [450/980], Loss: 6.3869\n",
      "Epoch [460/980], Loss: 5.8480\n",
      "Epoch [470/980], Loss: 6.0777\n",
      "Epoch [480/980], Loss: 6.4817\n",
      "Epoch [490/980], Loss: 3.2235\n",
      "Epoch [500/980], Loss: 6.3607\n",
      "Epoch [510/980], Loss: 5.4649\n",
      "Epoch [520/980], Loss: 3.7925\n",
      "Epoch [530/980], Loss: 4.4663\n",
      "Epoch [540/980], Loss: 4.2648\n",
      "Epoch [550/980], Loss: 4.1913\n",
      "Epoch [560/980], Loss: 4.7860\n",
      "Epoch [570/980], Loss: 2.6814\n",
      "Epoch [580/980], Loss: 3.0075\n",
      "Epoch [590/980], Loss: 3.4798\n",
      "Epoch [600/980], Loss: 1.4413\n",
      "Epoch [610/980], Loss: 1.5677\n",
      "Epoch [620/980], Loss: 1.9624\n",
      "Epoch [630/980], Loss: 2.8503\n",
      "Epoch [640/980], Loss: 2.9510\n",
      "Epoch [650/980], Loss: 3.2979\n",
      "Epoch [660/980], Loss: 2.9345\n",
      "Epoch [670/980], Loss: 3.2530\n",
      "Epoch [680/980], Loss: 2.4043\n",
      "Epoch [690/980], Loss: 2.1481\n",
      "Epoch [700/980], Loss: 2.6935\n",
      "Epoch [710/980], Loss: 2.1511\n",
      "Epoch [720/980], Loss: 2.5498\n",
      "Epoch [730/980], Loss: 3.0279\n",
      "Epoch [740/980], Loss: 2.1818\n",
      "Epoch [750/980], Loss: 2.5788\n",
      "Epoch [760/980], Loss: 1.9794\n",
      "Epoch [770/980], Loss: 1.7115\n",
      "Epoch [780/980], Loss: 2.2255\n",
      "Epoch [790/980], Loss: 2.0698\n",
      "Epoch [800/980], Loss: 1.3051\n",
      "Epoch [810/980], Loss: 2.0334\n",
      "Epoch [820/980], Loss: 1.3659\n",
      "Epoch [830/980], Loss: 2.0876\n",
      "Epoch [840/980], Loss: 1.2262\n",
      "Epoch [850/980], Loss: 1.1390\n",
      "Epoch [860/980], Loss: 1.7343\n",
      "Epoch [870/980], Loss: 1.7220\n",
      "Epoch [880/980], Loss: 2.0716\n",
      "Epoch [890/980], Loss: 1.6244\n",
      "Epoch [900/980], Loss: 1.5074\n",
      "Epoch [910/980], Loss: 0.8385\n",
      "Epoch [920/980], Loss: 1.1958\n",
      "Epoch [930/980], Loss: 1.4067\n",
      "Epoch [940/980], Loss: 1.4271\n",
      "Epoch [950/980], Loss: 1.5835\n",
      "Epoch [960/980], Loss: 0.9635\n",
      "Epoch [970/980], Loss: 1.7335\n",
      "Epoch [980/980], Loss: 1.2526\n"
     ]
    }
   ],
   "source": [
    "# Treinamos o modelo por 100 epocas\n",
    "fit(980, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.0504,  70.4346],\n",
       "        [ 81.6696, 100.2610],\n",
       "        [118.8812, 133.4099],\n",
       "        [ 20.9936,  37.9197],\n",
       "        [101.1947, 117.9174],\n",
       "        [ 55.7939,  69.3501],\n",
       "        [ 81.4629, 100.3039],\n",
       "        [119.1373, 133.9831],\n",
       "        [ 22.2501,  39.0042],\n",
       "        [102.2445, 119.0448],\n",
       "        [ 56.8437,  70.4774],\n",
       "        [ 80.4131,  99.1765],\n",
       "        [119.0879, 133.3670],\n",
       "        [ 19.9438,  36.7923],\n",
       "        [102.4512, 119.0019]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Geramos as predições\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 57.,  69.],\n",
       "        [ 80., 102.],\n",
       "        [118., 132.],\n",
       "        [ 21.,  38.],\n",
       "        [104., 118.],\n",
       "        [ 57.,  69.],\n",
       "        [ 82., 100.],\n",
       "        [118., 134.],\n",
       "        [ 20.,  38.],\n",
       "        [102., 120.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparando com os targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[53.4712, 67.5010]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predizemos novos valores\n",
    "model(torch.tensor([[75, 63, 44.]]))\n",
    "\n",
    "# -- Resposta --\n",
    "# tensor([[53.4258, 67.6156]], grad_fn=<AddmmBackward0>)\n",
    "\n",
    "# Significa que o modelo prevê 53 maçãs e 67 laranjas, arredondando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = nn.Sequential(\n",
    "    nn.Linear(3, 4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model2.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/5000], Loss: 6825.5259\n",
      "Epoch [20/5000], Loss: 9764.5771\n",
      "Epoch [30/5000], Loss: 6742.0459\n",
      "Epoch [40/5000], Loss: 6686.0259\n",
      "Epoch [50/5000], Loss: 8116.0249\n",
      "Epoch [60/5000], Loss: 10204.5039\n",
      "Epoch [70/5000], Loss: 9455.8799\n",
      "Epoch [80/5000], Loss: 9337.6172\n",
      "Epoch [90/5000], Loss: 12326.6982\n",
      "Epoch [100/5000], Loss: 11336.5801\n",
      "Epoch [110/5000], Loss: 6776.5259\n",
      "Epoch [120/5000], Loss: 6893.5537\n",
      "Epoch [130/5000], Loss: 5362.0591\n",
      "Epoch [140/5000], Loss: 6854.4297\n",
      "Epoch [150/5000], Loss: 6078.3486\n",
      "Epoch [160/5000], Loss: 5124.6064\n",
      "Epoch [170/5000], Loss: 8617.7246\n",
      "Epoch [180/5000], Loss: 3577.6250\n",
      "Epoch [190/5000], Loss: 8998.0996\n",
      "Epoch [200/5000], Loss: 4158.7769\n",
      "Epoch [210/5000], Loss: 9490.4229\n",
      "Epoch [220/5000], Loss: 2558.2302\n",
      "Epoch [230/5000], Loss: 5334.0186\n",
      "Epoch [240/5000], Loss: 7112.1235\n",
      "Epoch [250/5000], Loss: 3554.5122\n",
      "Epoch [260/5000], Loss: 6866.6196\n",
      "Epoch [270/5000], Loss: 3684.4187\n",
      "Epoch [280/5000], Loss: 10446.1904\n",
      "Epoch [290/5000], Loss: 5547.8613\n",
      "Epoch [300/5000], Loss: 4588.6377\n",
      "Epoch [310/5000], Loss: 6486.3677\n",
      "Epoch [320/5000], Loss: 3372.2593\n",
      "Epoch [330/5000], Loss: 4742.2573\n",
      "Epoch [340/5000], Loss: 8679.0938\n",
      "Epoch [350/5000], Loss: 4872.7954\n",
      "Epoch [360/5000], Loss: 7045.0264\n",
      "Epoch [370/5000], Loss: 3105.8721\n",
      "Epoch [380/5000], Loss: 6430.2124\n",
      "Epoch [390/5000], Loss: 3000.8345\n",
      "Epoch [400/5000], Loss: 3895.1538\n",
      "Epoch [410/5000], Loss: 2814.3713\n",
      "Epoch [420/5000], Loss: 3165.7314\n",
      "Epoch [430/5000], Loss: 6950.2632\n",
      "Epoch [440/5000], Loss: 6336.0786\n",
      "Epoch [450/5000], Loss: 5284.4707\n",
      "Epoch [460/5000], Loss: 7326.5366\n",
      "Epoch [470/5000], Loss: 4057.7495\n",
      "Epoch [480/5000], Loss: 3405.1399\n",
      "Epoch [490/5000], Loss: 1961.7256\n",
      "Epoch [500/5000], Loss: 4191.0898\n",
      "Epoch [510/5000], Loss: 5544.9424\n",
      "Epoch [520/5000], Loss: 5718.7852\n",
      "Epoch [530/5000], Loss: 5073.8604\n",
      "Epoch [540/5000], Loss: 4803.8760\n",
      "Epoch [550/5000], Loss: 5994.6240\n",
      "Epoch [560/5000], Loss: 3592.9058\n",
      "Epoch [570/5000], Loss: 3870.1375\n",
      "Epoch [580/5000], Loss: 2908.8196\n",
      "Epoch [590/5000], Loss: 3937.3618\n",
      "Epoch [600/5000], Loss: 5456.3716\n",
      "Epoch [610/5000], Loss: 6121.6650\n",
      "Epoch [620/5000], Loss: 6051.8662\n",
      "Epoch [630/5000], Loss: 2070.1541\n",
      "Epoch [640/5000], Loss: 1553.7396\n",
      "Epoch [650/5000], Loss: 4168.3613\n",
      "Epoch [660/5000], Loss: 2976.1694\n",
      "Epoch [670/5000], Loss: 4066.3372\n",
      "Epoch [680/5000], Loss: 3151.7896\n",
      "Epoch [690/5000], Loss: 3486.9199\n",
      "Epoch [700/5000], Loss: 4026.6294\n",
      "Epoch [710/5000], Loss: 3279.3989\n",
      "Epoch [720/5000], Loss: 3246.5554\n",
      "Epoch [730/5000], Loss: 3463.0430\n",
      "Epoch [740/5000], Loss: 6277.3921\n",
      "Epoch [750/5000], Loss: 4156.0029\n",
      "Epoch [760/5000], Loss: 3684.5874\n",
      "Epoch [770/5000], Loss: 4597.8989\n",
      "Epoch [780/5000], Loss: 5053.6299\n",
      "Epoch [790/5000], Loss: 1690.7614\n",
      "Epoch [800/5000], Loss: 4911.4165\n",
      "Epoch [810/5000], Loss: 1652.5149\n",
      "Epoch [820/5000], Loss: 2119.2827\n",
      "Epoch [830/5000], Loss: 1144.2441\n",
      "Epoch [840/5000], Loss: 2511.0679\n",
      "Epoch [850/5000], Loss: 2051.4390\n",
      "Epoch [860/5000], Loss: 2059.6206\n",
      "Epoch [870/5000], Loss: 2424.3130\n",
      "Epoch [880/5000], Loss: 2495.5742\n",
      "Epoch [890/5000], Loss: 4461.1509\n",
      "Epoch [900/5000], Loss: 3196.9993\n",
      "Epoch [910/5000], Loss: 3589.5161\n",
      "Epoch [920/5000], Loss: 3101.4006\n",
      "Epoch [930/5000], Loss: 3917.4165\n",
      "Epoch [940/5000], Loss: 3026.4924\n",
      "Epoch [950/5000], Loss: 1434.4792\n",
      "Epoch [960/5000], Loss: 2967.4377\n",
      "Epoch [970/5000], Loss: 961.5558\n",
      "Epoch [980/5000], Loss: 611.2242\n",
      "Epoch [990/5000], Loss: 3684.2539\n",
      "Epoch [1000/5000], Loss: 3243.8591\n",
      "Epoch [1010/5000], Loss: 1357.1499\n",
      "Epoch [1020/5000], Loss: 2156.2334\n",
      "Epoch [1030/5000], Loss: 2078.9185\n",
      "Epoch [1040/5000], Loss: 2402.6973\n",
      "Epoch [1050/5000], Loss: 2680.6062\n",
      "Epoch [1060/5000], Loss: 1981.0281\n",
      "Epoch [1070/5000], Loss: 1296.9738\n",
      "Epoch [1080/5000], Loss: 2039.3405\n",
      "Epoch [1090/5000], Loss: 3365.7476\n",
      "Epoch [1100/5000], Loss: 1879.3289\n",
      "Epoch [1110/5000], Loss: 2213.7771\n",
      "Epoch [1120/5000], Loss: 1553.5820\n",
      "Epoch [1130/5000], Loss: 1492.0848\n",
      "Epoch [1140/5000], Loss: 2236.2607\n",
      "Epoch [1150/5000], Loss: 2383.9553\n",
      "Epoch [1160/5000], Loss: 2095.7651\n",
      "Epoch [1170/5000], Loss: 1400.1528\n",
      "Epoch [1180/5000], Loss: 1888.5771\n",
      "Epoch [1190/5000], Loss: 828.9581\n",
      "Epoch [1200/5000], Loss: 2227.9421\n",
      "Epoch [1210/5000], Loss: 2001.4587\n",
      "Epoch [1220/5000], Loss: 1075.5371\n",
      "Epoch [1230/5000], Loss: 1187.0576\n",
      "Epoch [1240/5000], Loss: 1408.3519\n",
      "Epoch [1250/5000], Loss: 2521.7622\n",
      "Epoch [1260/5000], Loss: 2511.8931\n",
      "Epoch [1270/5000], Loss: 442.8035\n",
      "Epoch [1280/5000], Loss: 2440.1924\n",
      "Epoch [1290/5000], Loss: 2933.1440\n",
      "Epoch [1300/5000], Loss: 1733.0740\n",
      "Epoch [1310/5000], Loss: 2196.6602\n",
      "Epoch [1320/5000], Loss: 2889.8826\n",
      "Epoch [1330/5000], Loss: 2696.2163\n",
      "Epoch [1340/5000], Loss: 1554.0012\n",
      "Epoch [1350/5000], Loss: 1750.7797\n",
      "Epoch [1360/5000], Loss: 1887.7625\n",
      "Epoch [1370/5000], Loss: 2739.6050\n",
      "Epoch [1380/5000], Loss: 1865.4906\n",
      "Epoch [1390/5000], Loss: 1498.0393\n",
      "Epoch [1400/5000], Loss: 1490.5217\n",
      "Epoch [1410/5000], Loss: 1297.3041\n",
      "Epoch [1420/5000], Loss: 3045.4138\n",
      "Epoch [1430/5000], Loss: 1437.6259\n",
      "Epoch [1440/5000], Loss: 1225.5448\n",
      "Epoch [1450/5000], Loss: 1792.4506\n",
      "Epoch [1460/5000], Loss: 2108.0850\n",
      "Epoch [1470/5000], Loss: 1097.2786\n",
      "Epoch [1480/5000], Loss: 1858.4629\n",
      "Epoch [1490/5000], Loss: 1211.2240\n",
      "Epoch [1500/5000], Loss: 2560.6099\n",
      "Epoch [1510/5000], Loss: 1840.9977\n",
      "Epoch [1520/5000], Loss: 2372.0566\n",
      "Epoch [1530/5000], Loss: 2519.2651\n",
      "Epoch [1540/5000], Loss: 2224.3430\n",
      "Epoch [1550/5000], Loss: 694.5770\n",
      "Epoch [1560/5000], Loss: 1634.1139\n",
      "Epoch [1570/5000], Loss: 690.6666\n",
      "Epoch [1580/5000], Loss: 1979.1061\n",
      "Epoch [1590/5000], Loss: 1292.1888\n",
      "Epoch [1600/5000], Loss: 2049.0178\n",
      "Epoch [1610/5000], Loss: 2429.9512\n",
      "Epoch [1620/5000], Loss: 2156.8542\n",
      "Epoch [1630/5000], Loss: 1311.3636\n",
      "Epoch [1640/5000], Loss: 1945.3887\n",
      "Epoch [1650/5000], Loss: 1142.6215\n",
      "Epoch [1660/5000], Loss: 2227.0383\n",
      "Epoch [1670/5000], Loss: 2028.4622\n",
      "Epoch [1680/5000], Loss: 1150.6510\n",
      "Epoch [1690/5000], Loss: 1542.0247\n",
      "Epoch [1700/5000], Loss: 286.0649\n",
      "Epoch [1710/5000], Loss: 2180.6523\n",
      "Epoch [1720/5000], Loss: 1696.7688\n",
      "Epoch [1730/5000], Loss: 1979.0387\n",
      "Epoch [1740/5000], Loss: 2538.4155\n",
      "Epoch [1750/5000], Loss: 2242.9070\n",
      "Epoch [1760/5000], Loss: 1996.2971\n",
      "Epoch [1770/5000], Loss: 2263.5957\n",
      "Epoch [1780/5000], Loss: 1119.6807\n",
      "Epoch [1790/5000], Loss: 1399.3722\n",
      "Epoch [1800/5000], Loss: 1508.6580\n",
      "Epoch [1810/5000], Loss: 2376.8982\n",
      "Epoch [1820/5000], Loss: 1785.0662\n",
      "Epoch [1830/5000], Loss: 1492.0049\n",
      "Epoch [1840/5000], Loss: 1480.8514\n",
      "Epoch [1850/5000], Loss: 913.5778\n",
      "Epoch [1860/5000], Loss: 2052.3096\n",
      "Epoch [1870/5000], Loss: 1938.2874\n",
      "Epoch [1880/5000], Loss: 608.7601\n",
      "Epoch [1890/5000], Loss: 2318.8357\n",
      "Epoch [1900/5000], Loss: 902.0125\n",
      "Epoch [1910/5000], Loss: 1201.3030\n",
      "Epoch [1920/5000], Loss: 1725.3582\n",
      "Epoch [1930/5000], Loss: 1087.0978\n",
      "Epoch [1940/5000], Loss: 1982.2223\n",
      "Epoch [1950/5000], Loss: 577.0116\n",
      "Epoch [1960/5000], Loss: 1962.7448\n",
      "Epoch [1970/5000], Loss: 315.2617\n",
      "Epoch [1980/5000], Loss: 1118.6877\n",
      "Epoch [1990/5000], Loss: 1304.0588\n",
      "Epoch [2000/5000], Loss: 1685.0032\n",
      "Epoch [2010/5000], Loss: 527.4959\n",
      "Epoch [2020/5000], Loss: 1936.1686\n",
      "Epoch [2030/5000], Loss: 1420.6705\n",
      "Epoch [2040/5000], Loss: 1617.8970\n",
      "Epoch [2050/5000], Loss: 1136.8123\n",
      "Epoch [2060/5000], Loss: 1389.8735\n",
      "Epoch [2070/5000], Loss: 1094.7032\n",
      "Epoch [2080/5000], Loss: 2503.4746\n",
      "Epoch [2090/5000], Loss: 457.5184\n",
      "Epoch [2100/5000], Loss: 839.3433\n",
      "Epoch [2110/5000], Loss: 1352.0320\n",
      "Epoch [2120/5000], Loss: 1058.7365\n",
      "Epoch [2130/5000], Loss: 1106.6536\n",
      "Epoch [2140/5000], Loss: 1295.5670\n",
      "Epoch [2150/5000], Loss: 2203.5012\n",
      "Epoch [2160/5000], Loss: 1086.6512\n",
      "Epoch [2170/5000], Loss: 1336.3612\n",
      "Epoch [2180/5000], Loss: 1361.6094\n",
      "Epoch [2190/5000], Loss: 1073.9833\n",
      "Epoch [2200/5000], Loss: 2180.2495\n",
      "Epoch [2210/5000], Loss: 1615.6443\n",
      "Epoch [2220/5000], Loss: 1347.7578\n",
      "Epoch [2230/5000], Loss: 1490.1827\n",
      "Epoch [2240/5000], Loss: 2108.9675\n",
      "Epoch [2250/5000], Loss: 1196.6843\n",
      "Epoch [2260/5000], Loss: 1287.5991\n",
      "Epoch [2270/5000], Loss: 1679.8226\n",
      "Epoch [2280/5000], Loss: 1610.3551\n",
      "Epoch [2290/5000], Loss: 1902.8298\n",
      "Epoch [2300/5000], Loss: 1445.5485\n",
      "Epoch [2310/5000], Loss: 1363.9578\n",
      "Epoch [2320/5000], Loss: 1749.5793\n",
      "Epoch [2330/5000], Loss: 1395.4607\n",
      "Epoch [2340/5000], Loss: 755.3427\n",
      "Epoch [2350/5000], Loss: 1802.0746\n",
      "Epoch [2360/5000], Loss: 1287.1578\n",
      "Epoch [2370/5000], Loss: 1815.3704\n",
      "Epoch [2380/5000], Loss: 1177.5614\n",
      "Epoch [2390/5000], Loss: 1289.8274\n",
      "Epoch [2400/5000], Loss: 1957.9128\n",
      "Epoch [2410/5000], Loss: 993.6566\n",
      "Epoch [2420/5000], Loss: 1779.6832\n",
      "Epoch [2430/5000], Loss: 1152.7866\n",
      "Epoch [2440/5000], Loss: 1549.9539\n",
      "Epoch [2450/5000], Loss: 1941.3281\n",
      "Epoch [2460/5000], Loss: 1545.4213\n",
      "Epoch [2470/5000], Loss: 567.6886\n",
      "Epoch [2480/5000], Loss: 907.2612\n",
      "Epoch [2490/5000], Loss: 1056.3417\n",
      "Epoch [2500/5000], Loss: 1294.0756\n",
      "Epoch [2510/5000], Loss: 880.1971\n",
      "Epoch [2520/5000], Loss: 634.5294\n",
      "Epoch [2530/5000], Loss: 1025.2489\n",
      "Epoch [2540/5000], Loss: 783.3122\n",
      "Epoch [2550/5000], Loss: 1742.4984\n",
      "Epoch [2560/5000], Loss: 886.7430\n",
      "Epoch [2570/5000], Loss: 1194.2786\n",
      "Epoch [2580/5000], Loss: 1531.2352\n",
      "Epoch [2590/5000], Loss: 1045.5934\n",
      "Epoch [2600/5000], Loss: 1060.1140\n",
      "Epoch [2610/5000], Loss: 671.0388\n",
      "Epoch [2620/5000], Loss: 1267.3380\n",
      "Epoch [2630/5000], Loss: 1950.2363\n",
      "Epoch [2640/5000], Loss: 1410.8223\n",
      "Epoch [2650/5000], Loss: 881.6206\n",
      "Epoch [2660/5000], Loss: 1255.9690\n",
      "Epoch [2670/5000], Loss: 1438.5413\n",
      "Epoch [2680/5000], Loss: 1069.9116\n",
      "Epoch [2690/5000], Loss: 793.4222\n",
      "Epoch [2700/5000], Loss: 1452.5518\n",
      "Epoch [2710/5000], Loss: 1047.6838\n",
      "Epoch [2720/5000], Loss: 797.2806\n",
      "Epoch [2730/5000], Loss: 1665.0980\n",
      "Epoch [2740/5000], Loss: 1260.3289\n",
      "Epoch [2750/5000], Loss: 804.7369\n",
      "Epoch [2760/5000], Loss: 842.5482\n",
      "Epoch [2770/5000], Loss: 1017.1422\n",
      "Epoch [2780/5000], Loss: 1687.2312\n",
      "Epoch [2790/5000], Loss: 1492.3064\n",
      "Epoch [2800/5000], Loss: 1256.7034\n",
      "Epoch [2810/5000], Loss: 1085.1545\n",
      "Epoch [2820/5000], Loss: 1224.0592\n",
      "Epoch [2830/5000], Loss: 1043.0509\n",
      "Epoch [2840/5000], Loss: 1646.6471\n",
      "Epoch [2850/5000], Loss: 1465.2284\n",
      "Epoch [2860/5000], Loss: 2072.0894\n",
      "Epoch [2870/5000], Loss: 993.1799\n",
      "Epoch [2880/5000], Loss: 1670.6184\n",
      "Epoch [2890/5000], Loss: 1664.8430\n",
      "Epoch [2900/5000], Loss: 1497.1926\n",
      "Epoch [2910/5000], Loss: 994.5879\n",
      "Epoch [2920/5000], Loss: 1460.9214\n",
      "Epoch [2930/5000], Loss: 1065.9304\n",
      "Epoch [2940/5000], Loss: 1250.1897\n",
      "Epoch [2950/5000], Loss: 1489.0701\n",
      "Epoch [2960/5000], Loss: 1242.0499\n",
      "Epoch [2970/5000], Loss: 1223.3829\n",
      "Epoch [2980/5000], Loss: 1213.2366\n",
      "Epoch [2990/5000], Loss: 1499.9871\n",
      "Epoch [3000/5000], Loss: 1503.2039\n",
      "Epoch [3010/5000], Loss: 1653.9434\n",
      "Epoch [3020/5000], Loss: 1209.5472\n",
      "Epoch [3030/5000], Loss: 1239.4475\n",
      "Epoch [3040/5000], Loss: 953.5853\n",
      "Epoch [3050/5000], Loss: 1227.2244\n",
      "Epoch [3060/5000], Loss: 1184.7610\n",
      "Epoch [3070/5000], Loss: 1245.9138\n",
      "Epoch [3080/5000], Loss: 1166.4084\n",
      "Epoch [3090/5000], Loss: 1027.5250\n",
      "Epoch [3100/5000], Loss: 1428.8680\n",
      "Epoch [3110/5000], Loss: 1462.2786\n",
      "Epoch [3120/5000], Loss: 771.4669\n",
      "Epoch [3130/5000], Loss: 965.2325\n",
      "Epoch [3140/5000], Loss: 1516.1055\n",
      "Epoch [3150/5000], Loss: 992.0349\n",
      "Epoch [3160/5000], Loss: 1240.0920\n",
      "Epoch [3170/5000], Loss: 846.2843\n",
      "Epoch [3180/5000], Loss: 1788.1090\n",
      "Epoch [3190/5000], Loss: 759.6660\n",
      "Epoch [3200/5000], Loss: 1454.2218\n",
      "Epoch [3210/5000], Loss: 1198.9846\n",
      "Epoch [3220/5000], Loss: 1195.5016\n",
      "Epoch [3230/5000], Loss: 928.8577\n",
      "Epoch [3240/5000], Loss: 1198.3162\n",
      "Epoch [3250/5000], Loss: 798.9707\n",
      "Epoch [3260/5000], Loss: 1388.7135\n",
      "Epoch [3270/5000], Loss: 1380.1274\n",
      "Epoch [3280/5000], Loss: 1603.1107\n",
      "Epoch [3290/5000], Loss: 1221.8083\n",
      "Epoch [3300/5000], Loss: 978.8627\n",
      "Epoch [3310/5000], Loss: 1240.5540\n",
      "Epoch [3320/5000], Loss: 1126.8645\n",
      "Epoch [3330/5000], Loss: 595.6873\n",
      "Epoch [3340/5000], Loss: 1052.3051\n",
      "Epoch [3350/5000], Loss: 1053.0154\n",
      "Epoch [3360/5000], Loss: 1192.4844\n",
      "Epoch [3370/5000], Loss: 1127.7296\n",
      "Epoch [3380/5000], Loss: 2010.6287\n",
      "Epoch [3390/5000], Loss: 1251.9795\n",
      "Epoch [3400/5000], Loss: 1220.3602\n",
      "Epoch [3410/5000], Loss: 1510.8959\n",
      "Epoch [3420/5000], Loss: 742.1658\n",
      "Epoch [3430/5000], Loss: 1220.2693\n",
      "Epoch [3440/5000], Loss: 1376.9087\n",
      "Epoch [3450/5000], Loss: 1041.5756\n",
      "Epoch [3460/5000], Loss: 1420.3627\n",
      "Epoch [3470/5000], Loss: 889.3703\n",
      "Epoch [3480/5000], Loss: 813.2878\n",
      "Epoch [3490/5000], Loss: 1311.4916\n",
      "Epoch [3500/5000], Loss: 953.7485\n",
      "Epoch [3510/5000], Loss: 1432.9812\n",
      "Epoch [3520/5000], Loss: 1206.4021\n",
      "Epoch [3530/5000], Loss: 1236.0160\n",
      "Epoch [3540/5000], Loss: 1188.1650\n",
      "Epoch [3550/5000], Loss: 1150.6930\n",
      "Epoch [3560/5000], Loss: 1188.4960\n",
      "Epoch [3570/5000], Loss: 1608.7175\n",
      "Epoch [3580/5000], Loss: 1247.5778\n",
      "Epoch [3590/5000], Loss: 728.9536\n",
      "Epoch [3600/5000], Loss: 852.7606\n",
      "Epoch [3610/5000], Loss: 1827.0876\n",
      "Epoch [3620/5000], Loss: 864.4141\n",
      "Epoch [3630/5000], Loss: 1204.1404\n",
      "Epoch [3640/5000], Loss: 1367.3640\n",
      "Epoch [3650/5000], Loss: 663.3605\n",
      "Epoch [3660/5000], Loss: 1358.5142\n",
      "Epoch [3670/5000], Loss: 1279.0492\n",
      "Epoch [3680/5000], Loss: 1185.2855\n",
      "Epoch [3690/5000], Loss: 1675.9186\n",
      "Epoch [3700/5000], Loss: 1388.2971\n",
      "Epoch [3710/5000], Loss: 1332.8591\n",
      "Epoch [3720/5000], Loss: 1441.9408\n",
      "Epoch [3730/5000], Loss: 1875.3914\n",
      "Epoch [3740/5000], Loss: 1160.2581\n",
      "Epoch [3750/5000], Loss: 1318.2598\n",
      "Epoch [3760/5000], Loss: 1693.5293\n",
      "Epoch [3770/5000], Loss: 819.6051\n",
      "Epoch [3780/5000], Loss: 1216.5740\n",
      "Epoch [3790/5000], Loss: 1567.5157\n",
      "Epoch [3800/5000], Loss: 832.0110\n",
      "Epoch [3810/5000], Loss: 1343.1940\n",
      "Epoch [3820/5000], Loss: 1298.1116\n",
      "Epoch [3830/5000], Loss: 875.0485\n",
      "Epoch [3840/5000], Loss: 1661.0889\n",
      "Epoch [3850/5000], Loss: 687.1092\n",
      "Epoch [3860/5000], Loss: 1211.2864\n",
      "Epoch [3870/5000], Loss: 1046.9757\n",
      "Epoch [3880/5000], Loss: 1182.3186\n",
      "Epoch [3890/5000], Loss: 1513.5769\n",
      "Epoch [3900/5000], Loss: 1183.6714\n",
      "Epoch [3910/5000], Loss: 1044.5538\n",
      "Epoch [3920/5000], Loss: 724.6681\n",
      "Epoch [3930/5000], Loss: 1541.1810\n",
      "Epoch [3940/5000], Loss: 817.8131\n",
      "Epoch [3950/5000], Loss: 2030.9543\n",
      "Epoch [3960/5000], Loss: 1347.9960\n",
      "Epoch [3970/5000], Loss: 678.6844\n",
      "Epoch [3980/5000], Loss: 1345.4901\n",
      "Epoch [3990/5000], Loss: 1568.6046\n",
      "Epoch [4000/5000], Loss: 1402.9485\n",
      "Epoch [4010/5000], Loss: 1210.0588\n",
      "Epoch [4020/5000], Loss: 725.9924\n",
      "Epoch [4030/5000], Loss: 1049.4613\n",
      "Epoch [4040/5000], Loss: 1191.9132\n",
      "Epoch [4050/5000], Loss: 1147.5095\n",
      "Epoch [4060/5000], Loss: 672.8012\n",
      "Epoch [4070/5000], Loss: 1250.2302\n",
      "Epoch [4080/5000], Loss: 874.0225\n",
      "Epoch [4090/5000], Loss: 822.6525\n",
      "Epoch [4100/5000], Loss: 1401.8582\n",
      "Epoch [4110/5000], Loss: 878.8744\n",
      "Epoch [4120/5000], Loss: 826.1354\n",
      "Epoch [4130/5000], Loss: 447.4399\n",
      "Epoch [4140/5000], Loss: 880.2580\n",
      "Epoch [4150/5000], Loss: 1498.6217\n",
      "Epoch [4160/5000], Loss: 1037.8405\n",
      "Epoch [4170/5000], Loss: 1369.9441\n",
      "Epoch [4180/5000], Loss: 1358.4559\n",
      "Epoch [4190/5000], Loss: 1358.2449\n",
      "Epoch [4200/5000], Loss: 883.5415\n",
      "Epoch [4210/5000], Loss: 1470.9680\n",
      "Epoch [4220/5000], Loss: 1755.0771\n",
      "Epoch [4230/5000], Loss: 921.5303\n",
      "Epoch [4240/5000], Loss: 1029.6311\n",
      "Epoch [4250/5000], Loss: 718.3797\n",
      "Epoch [4260/5000], Loss: 1196.2174\n",
      "Epoch [4270/5000], Loss: 1592.8191\n",
      "Epoch [4280/5000], Loss: 1453.9646\n",
      "Epoch [4290/5000], Loss: 1196.6564\n",
      "Epoch [4300/5000], Loss: 978.3251\n",
      "Epoch [4310/5000], Loss: 1195.6504\n",
      "Epoch [4320/5000], Loss: 858.8192\n",
      "Epoch [4330/5000], Loss: 1717.8728\n",
      "Epoch [4340/5000], Loss: 807.2318\n",
      "Epoch [4350/5000], Loss: 1207.8518\n",
      "Epoch [4360/5000], Loss: 948.5151\n",
      "Epoch [4370/5000], Loss: 1195.4829\n",
      "Epoch [4380/5000], Loss: 1679.1041\n",
      "Epoch [4390/5000], Loss: 1061.9150\n",
      "Epoch [4400/5000], Loss: 1195.6458\n",
      "Epoch [4410/5000], Loss: 2037.5254\n",
      "Epoch [4420/5000], Loss: 1737.8871\n",
      "Epoch [4430/5000], Loss: 1343.4475\n",
      "Epoch [4440/5000], Loss: 1000.0673\n",
      "Epoch [4450/5000], Loss: 1386.3744\n",
      "Epoch [4460/5000], Loss: 1136.1490\n",
      "Epoch [4470/5000], Loss: 1326.3448\n",
      "Epoch [4480/5000], Loss: 798.2194\n",
      "Epoch [4490/5000], Loss: 649.9202\n",
      "Epoch [4500/5000], Loss: 563.5198\n",
      "Epoch [4510/5000], Loss: 709.8762\n",
      "Epoch [4520/5000], Loss: 1628.1306\n",
      "Epoch [4530/5000], Loss: 1874.1736\n",
      "Epoch [4540/5000], Loss: 1342.2914\n",
      "Epoch [4550/5000], Loss: 1187.6853\n",
      "Epoch [4560/5000], Loss: 1390.7213\n",
      "Epoch [4570/5000], Loss: 1537.3842\n",
      "Epoch [4580/5000], Loss: 1341.7058\n",
      "Epoch [4590/5000], Loss: 1323.7906\n",
      "Epoch [4600/5000], Loss: 1674.6774\n",
      "Epoch [4610/5000], Loss: 843.6431\n",
      "Epoch [4620/5000], Loss: 1821.6133\n",
      "Epoch [4630/5000], Loss: 1351.5699\n",
      "Epoch [4640/5000], Loss: 1173.7252\n",
      "Epoch [4650/5000], Loss: 750.4721\n",
      "Epoch [4660/5000], Loss: 1059.2988\n",
      "Epoch [4670/5000], Loss: 843.0462\n",
      "Epoch [4680/5000], Loss: 1176.8094\n",
      "Epoch [4690/5000], Loss: 1193.0605\n",
      "Epoch [4700/5000], Loss: 1757.0322\n",
      "Epoch [4710/5000], Loss: 1699.4060\n",
      "Epoch [4720/5000], Loss: 1316.9087\n",
      "Epoch [4730/5000], Loss: 1531.7059\n",
      "Epoch [4740/5000], Loss: 786.4824\n",
      "Epoch [4750/5000], Loss: 1822.0449\n",
      "Epoch [4760/5000], Loss: 1549.2402\n",
      "Epoch [4770/5000], Loss: 1339.1624\n",
      "Epoch [4780/5000], Loss: 1204.4192\n",
      "Epoch [4790/5000], Loss: 1466.9651\n",
      "Epoch [4800/5000], Loss: 705.8527\n",
      "Epoch [4810/5000], Loss: 639.9718\n",
      "Epoch [4820/5000], Loss: 1467.2268\n",
      "Epoch [4830/5000], Loss: 1535.3694\n",
      "Epoch [4840/5000], Loss: 1247.5310\n",
      "Epoch [4850/5000], Loss: 1076.7887\n",
      "Epoch [4860/5000], Loss: 1946.6547\n",
      "Epoch [4870/5000], Loss: 833.9847\n",
      "Epoch [4880/5000], Loss: 1049.3621\n",
      "Epoch [4890/5000], Loss: 1417.8040\n",
      "Epoch [4900/5000], Loss: 716.3027\n",
      "Epoch [4910/5000], Loss: 765.0659\n",
      "Epoch [4920/5000], Loss: 1607.4612\n",
      "Epoch [4930/5000], Loss: 1896.2781\n",
      "Epoch [4940/5000], Loss: 779.1009\n",
      "Epoch [4950/5000], Loss: 701.8138\n",
      "Epoch [4960/5000], Loss: 1139.2076\n",
      "Epoch [4970/5000], Loss: 1478.2543\n",
      "Epoch [4980/5000], Loss: 1763.5027\n",
      "Epoch [4990/5000], Loss: 916.1517\n",
      "Epoch [5000/5000], Loss: 1260.2889\n"
     ]
    }
   ],
   "source": [
    "fit(5000, model2, F.mse_loss, optimizer, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[75.1097, 90.9135]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.],\n",
      "        [ 57.,  69.],\n",
      "        [ 80., 102.],\n",
      "        [118., 132.],\n",
      "        [ 21.,  38.],\n",
      "        [104., 118.],\n",
      "        [ 57.,  69.],\n",
      "        [ 82., 100.],\n",
      "        [118., 134.],\n",
      "        [ 20.,  38.],\n",
      "        [102., 120.]])\n"
     ]
    }
   ],
   "source": [
    "print(model2(torch.tensor([[75, 63, 44.]])))\n",
    "print(targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
