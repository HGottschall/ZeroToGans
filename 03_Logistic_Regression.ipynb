{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse curso, usaremos a base de dados do MNIST para aprender sobre CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixamos o dataset\n",
    "dataset = MNIST(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/', train=False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28>, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d98f16b210>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mostramos a imagem\n",
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch não é uma biblioteca para processamento de imagens, e sim de tensores. Portanto, precisamos converter a imagem em tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root=\"data/\", train=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)\n",
    "# torch.Size([1, 28, 28]) 5 --> 1 canal de cor, 28x28 pixels que representa o 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[:, 10:15, 10:15])\n",
    "print(torch.max(img_tensor), torch.min(img_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d98f38b050>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_tensor[0, 10:15, 10:15], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento e Validação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir modelos para aplicações reais de aprendizado de máquina, precisamos separar o dataset em 3 partes:\n",
    "\n",
    "1. Treinamento: Usado para o treinamento do modelo;\n",
    "2. Validação: Usado para validar o modelo durante o treinamento;\n",
    "3. Teste: Usado para comparar diferentes resultados de modelos diferentes e medir a acurácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 28 * 28\n",
    "num_classes = 10\n",
    "\n",
    "# Modelo de regressão logística\n",
    "model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0283, -0.0279,  0.0056,  ..., -0.0185,  0.0138,  0.0192],\n",
       "        [-0.0312,  0.0345,  0.0008,  ..., -0.0277,  0.0237,  0.0099],\n",
       "        [ 0.0083, -0.0040,  0.0048,  ..., -0.0193, -0.0090,  0.0295],\n",
       "        ...,\n",
       "        [-0.0016, -0.0101,  0.0286,  ..., -0.0005,  0.0006,  0.0224],\n",
       "        [ 0.0121, -0.0317,  0.0271,  ..., -0.0012, -0.0306,  0.0238],\n",
       "        [-0.0344,  0.0188, -0.0202,  ..., -0.0310, -0.0349, -0.0347]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 2, 4, 9, 0, 1, 1, 1, 3, 2, 7, 2, 0, 0, 6, 7, 8, 6, 4, 9, 4, 8, 6, 5,\n",
      "        0, 6, 8, 9, 2, 9, 9, 6, 0, 6, 0, 5, 2, 6, 7, 6, 2, 4, 7, 5, 1, 2, 1, 1,\n",
      "        2, 5, 2, 5, 1, 5, 9, 5, 0, 6, 8, 8, 2, 2, 3, 4, 1, 1, 4, 2, 9, 7, 0, 6,\n",
      "        0, 4, 4, 4, 7, 2, 1, 3, 7, 0, 7, 0, 0, 6, 4, 6, 5, 2, 3, 3, 2, 6, 1, 8,\n",
      "        3, 5, 0, 0, 2, 0, 7, 2, 2, 6, 4, 5, 5, 6, 9, 7, 4, 6, 0, 1, 5, 6, 6, 4,\n",
      "        7, 4, 8, 3, 1, 2, 8, 6])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    images = images.reshape(batch_size, 784)\n",
    "    outputs = model(images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro do método `__init__`, instanciamos os pesos e bias utilizando `nn.Module`. E dentro do método `forward`, passamos o tensor com os pixels da imagem 28x28 com uma única dimensão. Dessa forma temos [28x28] --> [1x784]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=10, bias=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape :  torch.Size([128, 10])\n",
      "Sample outputs :\n",
      " tensor([[-0.0128, -0.4917, -0.0947, -0.1276, -0.1845,  0.2005, -0.0763, -0.0688,\n",
      "          0.0631, -0.3109],\n",
      "        [ 0.1414, -0.2720, -0.0100, -0.1467, -0.2264,  0.2533, -0.0171, -0.2970,\n",
      "          0.2813,  0.1426]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    break\n",
    "\n",
    "print(\"outputs.shape : \", outputs.shape)\n",
    "print(\"Sample outputs :\\n\", outputs[:2].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades dos exemplos:\n",
      " tensor([[0.1085, 0.0672, 0.1000, 0.0967, 0.0914, 0.1343, 0.1018, 0.1026, 0.1170,\n",
      "         0.0805],\n",
      "        [0.1145, 0.0758, 0.0984, 0.0859, 0.0793, 0.1281, 0.0977, 0.0739, 0.1317,\n",
      "         0.1147]])\n",
      "Soma:  0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos a função softmax em cada linha\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "# Olhamos as probabilidades\n",
    "print(\"Probabilidades dos exemplos:\\n\", probs[:2].data)\n",
    "\n",
    "# Adicionamos as probabilidades em uma linha de saída\n",
    "print(\"Soma: \", torch.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para determinarmos a predição, simplesmente escolhemos o valor máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 8, 6, 6, 0, 8, 6, 3, 3, 4, 5, 0, 9, 0, 5, 5, 0, 0, 9, 5, 6, 8, 0, 5,\n",
      "        8, 6, 3, 8, 6, 4, 5, 6, 6, 9, 9, 0, 9, 0, 9, 5, 2, 0, 6, 5, 6, 5, 8, 2,\n",
      "        0, 6, 5, 5, 9, 0, 5, 6, 9, 6, 4, 5, 6, 6, 0, 5, 0, 5, 9, 4, 6, 9, 0, 5,\n",
      "        6, 9, 9, 8, 5, 5, 4, 5, 6, 9, 5, 4, 6, 6, 5, 9, 8, 2, 6, 6, 6, 0, 0, 6,\n",
      "        8, 9, 4, 9, 9, 6, 9, 6, 9, 5, 5, 0, 0, 5, 1, 9, 5, 9, 6, 0, 5, 5, 5, 6,\n",
      "        5, 0, 0, 5, 6, 8, 8, 5])\n",
      "tensor([0.1343, 0.1317, 0.1231, 0.1153, 0.1181, 0.1194, 0.1284, 0.1216, 0.1237,\n",
      "        0.1211, 0.1330, 0.1346, 0.1327, 0.1302, 0.1214, 0.1419, 0.1258, 0.1278,\n",
      "        0.1197, 0.1399, 0.1141, 0.1173, 0.1285, 0.1275, 0.1262, 0.1434, 0.1179,\n",
      "        0.1260, 0.1172, 0.1214, 0.1231, 0.1161, 0.1405, 0.1277, 0.1097, 0.1344,\n",
      "        0.1336, 0.1174, 0.1255, 0.1371, 0.1178, 0.1245, 0.1382, 0.1232, 0.1205,\n",
      "        0.1335, 0.1191, 0.1102, 0.1190, 0.1120, 0.1120, 0.1282, 0.1165, 0.1373,\n",
      "        0.1233, 0.1154, 0.1216, 0.1254, 0.1145, 0.1288, 0.1137, 0.1253, 0.1439,\n",
      "        0.1125, 0.1275, 0.1411, 0.1147, 0.1117, 0.1354, 0.1416, 0.1268, 0.1224,\n",
      "        0.1138, 0.1172, 0.1164, 0.1124, 0.1166, 0.1269, 0.1303, 0.1124, 0.1425,\n",
      "        0.1130, 0.1231, 0.1152, 0.1205, 0.1272, 0.1279, 0.1170, 0.1251, 0.1236,\n",
      "        0.1096, 0.1134, 0.1227, 0.1107, 0.1296, 0.1185, 0.1209, 0.1110, 0.1199,\n",
      "        0.1339, 0.1203, 0.1120, 0.1232, 0.1172, 0.1310, 0.1146, 0.1332, 0.1266,\n",
      "        0.1230, 0.1254, 0.1282, 0.1228, 0.1152, 0.1152, 0.1418, 0.1155, 0.1413,\n",
      "        0.1502, 0.1256, 0.1287, 0.1135, 0.1369, 0.1241, 0.1245, 0.1470, 0.1245,\n",
      "        0.1226, 0.1239], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação e Função de Perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    # Pega o índice do maior valor\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "    # Número de exemplos que foram classificados corretamente\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(preds == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0547)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2945, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Perda para a batch atual de dados\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.exp(torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1008, grad_fn=<PowBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(e**-loss)\n",
    "\n",
    "# O modelo tem 9% de probabilidade de classificar corretamente a imagem antes do treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo temos um pseudocódigo para o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0):\n",
    "    # Fase de treinamento\n",
    "    for batch in train_loader:\n",
    "        # Geramos previsões\n",
    "        # Calculamos a perda\n",
    "        # Computamos os gradientes\n",
    "        # Atualizamos os pesos\n",
    "        # Resetamos os gradientes\n",
    "        ...\n",
    "\n",
    "    # Fase de validação\n",
    "    for batch in val_loader:\n",
    "        # Geramos previsões\n",
    "        # Calculamos a perda\n",
    "        # Computamos a métrica de desempenho\n",
    "        ...\n",
    "\n",
    "    # Calculamos a média da perda e da métrica de desempenho em todas as batches de treinamento\n",
    "\n",
    "    # Registramos os valores de perda e métrica de desempenho para inspeção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    history = []  # Para armazenar os valores de perda e métrica de desempenho\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Fase de treinamento\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Fase de validação\n",
    "        result = []\n",
    "        for batch in val_loader:\n",
    "            result.append(model.validation_step(batch))\n",
    "        model.validation_epoch_end(result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de treinar, precisamos adicionar os métodos de treinamento e validação na classe `MnistModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        return self.linear(xb)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Gera previsões\n",
    "        return F.cross_entropy(out, labels)  # Calcula a perda\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Gera previsões\n",
    "        loss = F.cross_entropy(out, labels)  # Calcula a perda\n",
    "        acc = accuracy(out, labels)  # Calcula a acurácia\n",
    "        return {\"val_loss\": loss.detach(), \"val_acc\": acc.detach()}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x[\"val_loss\"] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combina as perdas\n",
    "        batch_accs = [x[\"val_acc\"] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combina as acurácias\n",
    "        return {\"val_loss\": epoch_loss.item(), \"val_acc\": epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\n",
    "            \"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "                epoch + 1, result[\"val_loss\"], result[\"val_acc\"]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.304379940032959, 'val_acc': 0.07298259437084198}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MnistModel()\n",
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'val_loss': tensor(1.9348), 'val_acc': tensor(0.6875)},\n",
       "  {'val_loss': tensor(1.9176), 'val_acc': tensor(0.7109)},\n",
       "  {'val_loss': tensor(1.9212), 'val_acc': tensor(0.6484)},\n",
       "  {'val_loss': tensor(1.9135), 'val_acc': tensor(0.6719)},\n",
       "  {'val_loss': tensor(1.9507), 'val_acc': tensor(0.6328)},\n",
       "  {'val_loss': tensor(1.9291), 'val_acc': tensor(0.6797)},\n",
       "  {'val_loss': tensor(1.9294), 'val_acc': tensor(0.6484)},\n",
       "  {'val_loss': tensor(1.9234), 'val_acc': tensor(0.6719)},\n",
       "  {'val_loss': tensor(1.9353), 'val_acc': tensor(0.6953)},\n",
       "  {'val_loss': tensor(1.9512), 'val_acc': tensor(0.5938)},\n",
       "  {'val_loss': tensor(1.9557), 'val_acc': tensor(0.6328)},\n",
       "  {'val_loss': tensor(1.8951), 'val_acc': tensor(0.6641)},\n",
       "  {'val_loss': tensor(1.9353), 'val_acc': tensor(0.6719)},\n",
       "  {'val_loss': tensor(1.9470), 'val_acc': tensor(0.6406)},\n",
       "  {'val_loss': tensor(1.9295), 'val_acc': tensor(0.6484)},\n",
       "  {'val_loss': tensor(1.9330), 'val_acc': tensor(0.6641)},\n",
       "  {'val_loss': tensor(1.9080), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.8901), 'val_acc': tensor(0.7188)},\n",
       "  {'val_loss': tensor(1.9167), 'val_acc': tensor(0.6406)},\n",
       "  {'val_loss': tensor(1.9016), 'val_acc': tensor(0.6719)},\n",
       "  {'val_loss': tensor(1.9171), 'val_acc': tensor(0.6797)},\n",
       "  {'val_loss': tensor(1.9226), 'val_acc': tensor(0.6797)},\n",
       "  {'val_loss': tensor(1.9244), 'val_acc': tensor(0.6719)},\n",
       "  {'val_loss': tensor(1.9335), 'val_acc': tensor(0.6250)},\n",
       "  {'val_loss': tensor(1.9612), 'val_acc': tensor(0.5938)},\n",
       "  {'val_loss': tensor(1.9512), 'val_acc': tensor(0.6016)},\n",
       "  {'val_loss': tensor(1.9167), 'val_acc': tensor(0.6641)},\n",
       "  {'val_loss': tensor(1.8740), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.9568), 'val_acc': tensor(0.6406)},\n",
       "  {'val_loss': tensor(1.9560), 'val_acc': tensor(0.5859)},\n",
       "  {'val_loss': tensor(1.9157), 'val_acc': tensor(0.6484)},\n",
       "  {'val_loss': tensor(1.9453), 'val_acc': tensor(0.6172)},\n",
       "  {'val_loss': tensor(1.8963), 'val_acc': tensor(0.6641)},\n",
       "  {'val_loss': tensor(1.9386), 'val_acc': tensor(0.6328)},\n",
       "  {'val_loss': tensor(1.8935), 'val_acc': tensor(0.6719)},\n",
       "  {'val_loss': tensor(1.9108), 'val_acc': tensor(0.6406)},\n",
       "  {'val_loss': tensor(1.9404), 'val_acc': tensor(0.6172)},\n",
       "  {'val_loss': tensor(1.9403), 'val_acc': tensor(0.6875)},\n",
       "  {'val_loss': tensor(1.9231), 'val_acc': tensor(0.6641)},\n",
       "  {'val_loss': tensor(1.9184), 'val_acc': tensor(0.6719)},\n",
       "  {'val_loss': tensor(1.9376), 'val_acc': tensor(0.6172)},\n",
       "  {'val_loss': tensor(1.9528), 'val_acc': tensor(0.6328)},\n",
       "  {'val_loss': tensor(1.9529), 'val_acc': tensor(0.6016)},\n",
       "  {'val_loss': tensor(1.9342), 'val_acc': tensor(0.7188)},\n",
       "  {'val_loss': tensor(1.9431), 'val_acc': tensor(0.5781)},\n",
       "  {'val_loss': tensor(1.9381), 'val_acc': tensor(0.6641)},\n",
       "  {'val_loss': tensor(1.9402), 'val_acc': tensor(0.6250)},\n",
       "  {'val_loss': tensor(1.9331), 'val_acc': tensor(0.6641)},\n",
       "  {'val_loss': tensor(1.8958), 'val_acc': tensor(0.7266)},\n",
       "  {'val_loss': tensor(1.9183), 'val_acc': tensor(0.7109)},\n",
       "  {'val_loss': tensor(1.9459), 'val_acc': tensor(0.6328)},\n",
       "  {'val_loss': tensor(1.8905), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.9227), 'val_acc': tensor(0.6484)},\n",
       "  {'val_loss': tensor(1.9456), 'val_acc': tensor(0.6562)},\n",
       "  {'val_loss': tensor(1.9218), 'val_acc': tensor(0.6641)},\n",
       "  {'val_loss': tensor(1.9425), 'val_acc': tensor(0.6562)},\n",
       "  {'val_loss': tensor(1.9451), 'val_acc': tensor(0.6484)},\n",
       "  {'val_loss': tensor(1.9756), 'val_acc': tensor(0.6172)},\n",
       "  {'val_loss': tensor(1.9028), 'val_acc': tensor(0.6484)},\n",
       "  {'val_loss': tensor(1.9048), 'val_acc': tensor(0.6875)},\n",
       "  {'val_loss': tensor(1.9280), 'val_acc': tensor(0.7188)},\n",
       "  {'val_loss': tensor(1.9572), 'val_acc': tensor(0.6484)},\n",
       "  {'val_loss': tensor(1.9238), 'val_acc': tensor(0.6719)},\n",
       "  {'val_loss': tensor(1.9417), 'val_acc': tensor(0.7109)},\n",
       "  {'val_loss': tensor(1.9157), 'val_acc': tensor(0.6797)},\n",
       "  {'val_loss': tensor(1.9494), 'val_acc': tensor(0.6328)},\n",
       "  {'val_loss': tensor(1.9194), 'val_acc': tensor(0.6484)},\n",
       "  {'val_loss': tensor(1.9471), 'val_acc': tensor(0.6484)},\n",
       "  {'val_loss': tensor(1.9514), 'val_acc': tensor(0.6562)},\n",
       "  {'val_loss': tensor(1.9554), 'val_acc': tensor(0.6328)},\n",
       "  {'val_loss': tensor(1.9139), 'val_acc': tensor(0.7188)},\n",
       "  {'val_loss': tensor(1.8940), 'val_acc': tensor(0.6797)},\n",
       "  {'val_loss': tensor(1.9673), 'val_acc': tensor(0.6641)},\n",
       "  {'val_loss': tensor(1.9425), 'val_acc': tensor(0.6328)},\n",
       "  {'val_loss': tensor(1.9318), 'val_acc': tensor(0.5625)},\n",
       "  {'val_loss': tensor(1.9340), 'val_acc': tensor(0.6641)},\n",
       "  {'val_loss': tensor(1.8862), 'val_acc': tensor(0.6875)},\n",
       "  {'val_loss': tensor(1.9375), 'val_acc': tensor(0.6406)},\n",
       "  {'val_loss': tensor(1.9800), 'val_acc': tensor(0.5000)}],\n",
       " [{'val_loss': tensor(1.6836), 'val_acc': tensor(0.7031)},\n",
       "  {'val_loss': tensor(1.6485), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.6664), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.6361), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.7019), 'val_acc': tensor(0.7188)},\n",
       "  {'val_loss': tensor(1.6681), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.6691), 'val_acc': tensor(0.7266)},\n",
       "  {'val_loss': tensor(1.6605), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.6811), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.6838), 'val_acc': tensor(0.6875)},\n",
       "  {'val_loss': tensor(1.7055), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.6320), 'val_acc': tensor(0.7266)},\n",
       "  {'val_loss': tensor(1.6623), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.6907), 'val_acc': tensor(0.6953)},\n",
       "  {'val_loss': tensor(1.6628), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.6768), 'val_acc': tensor(0.7188)},\n",
       "  {'val_loss': tensor(1.6219), 'val_acc': tensor(0.8359)},\n",
       "  {'val_loss': tensor(1.6050), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.6461), 'val_acc': tensor(0.6719)},\n",
       "  {'val_loss': tensor(1.6246), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.6468), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.6686), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.6405), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.6737), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.7095), 'val_acc': tensor(0.7109)},\n",
       "  {'val_loss': tensor(1.6638), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.6531), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.5955), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.7022), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.7051), 'val_acc': tensor(0.7031)},\n",
       "  {'val_loss': tensor(1.6430), 'val_acc': tensor(0.7188)},\n",
       "  {'val_loss': tensor(1.6933), 'val_acc': tensor(0.6641)},\n",
       "  {'val_loss': tensor(1.6099), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.6708), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.6254), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.6479), 'val_acc': tensor(0.7031)},\n",
       "  {'val_loss': tensor(1.6828), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.6767), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.6390), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.6463), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.6729), 'val_acc': tensor(0.7188)},\n",
       "  {'val_loss': tensor(1.6927), 'val_acc': tensor(0.7031)},\n",
       "  {'val_loss': tensor(1.7066), 'val_acc': tensor(0.6875)},\n",
       "  {'val_loss': tensor(1.6680), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.6736), 'val_acc': tensor(0.7031)},\n",
       "  {'val_loss': tensor(1.6748), 'val_acc': tensor(0.7188)},\n",
       "  {'val_loss': tensor(1.6829), 'val_acc': tensor(0.6953)},\n",
       "  {'val_loss': tensor(1.6673), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.6162), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.6480), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.6787), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.6017), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.6579), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.6911), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.6426), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.6981), 'val_acc': tensor(0.7109)},\n",
       "  {'val_loss': tensor(1.6873), 'val_acc': tensor(0.7031)},\n",
       "  {'val_loss': tensor(1.7339), 'val_acc': tensor(0.6406)},\n",
       "  {'val_loss': tensor(1.6256), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.6349), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.6621), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.7040), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.6420), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.6719), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.6329), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.6968), 'val_acc': tensor(0.6953)},\n",
       "  {'val_loss': tensor(1.6553), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.6769), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.6959), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.7146), 'val_acc': tensor(0.7266)},\n",
       "  {'val_loss': tensor(1.6394), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.6192), 'val_acc': tensor(0.7266)},\n",
       "  {'val_loss': tensor(1.7127), 'val_acc': tensor(0.7266)},\n",
       "  {'val_loss': tensor(1.6668), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.6785), 'val_acc': tensor(0.6875)},\n",
       "  {'val_loss': tensor(1.6756), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.6158), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.6611), 'val_acc': tensor(0.6953)},\n",
       "  {'val_loss': tensor(1.7495), 'val_acc': tensor(0.4375)}],\n",
       " [{'val_loss': tensor(1.4983), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.4492), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.4761), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.4259), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.5137), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.4728), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.4746), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.4678), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.4932), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.4842), 'val_acc': tensor(0.7109)},\n",
       "  {'val_loss': tensor(1.5162), 'val_acc': tensor(0.7266)},\n",
       "  {'val_loss': tensor(1.4385), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.4589), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.5004), 'val_acc': tensor(0.6953)},\n",
       "  {'val_loss': tensor(1.4643), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.4855), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.4145), 'val_acc': tensor(0.8281)},\n",
       "  {'val_loss': tensor(1.3978), 'val_acc': tensor(0.8359)},\n",
       "  {'val_loss': tensor(1.4445), 'val_acc': tensor(0.7109)},\n",
       "  {'val_loss': tensor(1.4182), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.4462), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.4788), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.4306), 'val_acc': tensor(0.8281)},\n",
       "  {'val_loss': tensor(1.4756), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.5206), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.4499), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.4599), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.3945), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.5097), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.5145), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.4404), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.5064), 'val_acc': tensor(0.7031)},\n",
       "  {'val_loss': tensor(1.4020), 'val_acc': tensor(0.8203)},\n",
       "  {'val_loss': tensor(1.4726), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.4265), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.4528), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.4873), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.4830), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.4341), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.4475), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.4766), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.4986), 'val_acc': tensor(0.7188)},\n",
       "  {'val_loss': tensor(1.5217), 'val_acc': tensor(0.7109)},\n",
       "  {'val_loss': tensor(1.4682), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.4724), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.4780), 'val_acc': tensor(0.7109)},\n",
       "  {'val_loss': tensor(1.4877), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.4700), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.4102), 'val_acc': tensor(0.8203)},\n",
       "  {'val_loss': tensor(1.4478), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.4781), 'val_acc': tensor(0.8203)},\n",
       "  {'val_loss': tensor(1.3915), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.4650), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.4958), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.4369), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.5163), 'val_acc': tensor(0.7266)},\n",
       "  {'val_loss': tensor(1.4932), 'val_acc': tensor(0.7109)},\n",
       "  {'val_loss': tensor(1.5523), 'val_acc': tensor(0.6719)},\n",
       "  {'val_loss': tensor(1.4230), 'val_acc': tensor(0.8125)},\n",
       "  {'val_loss': tensor(1.4371), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.4656), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.5089), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.4335), 'val_acc': tensor(0.8125)},\n",
       "  {'val_loss': tensor(1.4665), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.4256), 'val_acc': tensor(0.8359)},\n",
       "  {'val_loss': tensor(1.5072), 'val_acc': tensor(0.7266)},\n",
       "  {'val_loss': tensor(1.4577), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.4736), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.5027), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.5277), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.4346), 'val_acc': tensor(0.8359)},\n",
       "  {'val_loss': tensor(1.4139), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.5177), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.4611), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.4916), 'val_acc': tensor(0.7266)},\n",
       "  {'val_loss': tensor(1.4818), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.4177), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.4574), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.5697), 'val_acc': tensor(0.5000)}],\n",
       " [{'val_loss': tensor(1.3591), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.2997), 'val_acc': tensor(0.8438)},\n",
       "  {'val_loss': tensor(1.3314), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.2660), 'val_acc': tensor(0.8125)},\n",
       "  {'val_loss': tensor(1.3696), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.3243), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.3268), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.3236), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.3522), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.3319), 'val_acc': tensor(0.7266)},\n",
       "  {'val_loss': tensor(1.3701), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.2927), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.3068), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.3567), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.3152), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.3405), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.2613), 'val_acc': tensor(0.8359)},\n",
       "  {'val_loss': tensor(1.2446), 'val_acc': tensor(0.8594)},\n",
       "  {'val_loss': tensor(1.2929), 'val_acc': tensor(0.7188)},\n",
       "  {'val_loss': tensor(1.2636), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.2942), 'val_acc': tensor(0.8203)},\n",
       "  {'val_loss': tensor(1.3353), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.2727), 'val_acc': tensor(0.8594)},\n",
       "  {'val_loss': tensor(1.3231), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.3770), 'val_acc': tensor(0.7266)},\n",
       "  {'val_loss': tensor(1.2881), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.3154), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.2456), 'val_acc': tensor(0.8203)},\n",
       "  {'val_loss': tensor(1.3619), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.3673), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.2890), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.3649), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.2484), 'val_acc': tensor(0.8125)},\n",
       "  {'val_loss': tensor(1.3242), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.2757), 'val_acc': tensor(0.8203)},\n",
       "  {'val_loss': tensor(1.3050), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.3375), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.3369), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.2817), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.2973), 'val_acc': tensor(0.8438)},\n",
       "  {'val_loss': tensor(1.3286), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.3527), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.3817), 'val_acc': tensor(0.7266)},\n",
       "  {'val_loss': tensor(1.3167), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.3202), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.3301), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.3382), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.3212), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.2565), 'val_acc': tensor(0.8203)},\n",
       "  {'val_loss': tensor(1.2964), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.3249), 'val_acc': tensor(0.8203)},\n",
       "  {'val_loss': tensor(1.2356), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.3196), 'val_acc': tensor(0.8125)},\n",
       "  {'val_loss': tensor(1.3463), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.2839), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.3787), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.3461), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.4133), 'val_acc': tensor(0.6953)},\n",
       "  {'val_loss': tensor(1.2720), 'val_acc': tensor(0.8125)},\n",
       "  {'val_loss': tensor(1.2889), 'val_acc': tensor(0.8125)},\n",
       "  {'val_loss': tensor(1.3190), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.3585), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.2776), 'val_acc': tensor(0.8281)},\n",
       "  {'val_loss': tensor(1.3093), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.2692), 'val_acc': tensor(0.8594)},\n",
       "  {'val_loss': tensor(1.3630), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.3085), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.3186), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.3552), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.3828), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.2809), 'val_acc': tensor(0.8438)},\n",
       "  {'val_loss': tensor(1.2591), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.3676), 'val_acc': tensor(0.8125)},\n",
       "  {'val_loss': tensor(1.3055), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.3504), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.3338), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.2685), 'val_acc': tensor(0.8125)},\n",
       "  {'val_loss': tensor(1.3044), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.4296), 'val_acc': tensor(0.5625)}],\n",
       " [{'val_loss': tensor(1.2516), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.1844), 'val_acc': tensor(0.8516)},\n",
       "  {'val_loss': tensor(1.2196), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.1425), 'val_acc': tensor(0.8438)},\n",
       "  {'val_loss': tensor(1.2577), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.2092), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.2134), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.2137), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.2446), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.2147), 'val_acc': tensor(0.7500)},\n",
       "  {'val_loss': tensor(1.2562), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.1808), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.1895), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.2467), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.2007), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.2291), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.1450), 'val_acc': tensor(0.8438)},\n",
       "  {'val_loss': tensor(1.1272), 'val_acc': tensor(0.8672)},\n",
       "  {'val_loss': tensor(1.1781), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.1453), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.1771), 'val_acc': tensor(0.8281)},\n",
       "  {'val_loss': tensor(1.2253), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.1516), 'val_acc': tensor(0.8594)},\n",
       "  {'val_loss': tensor(1.2042), 'val_acc': tensor(0.8281)},\n",
       "  {'val_loss': tensor(1.2658), 'val_acc': tensor(0.7344)},\n",
       "  {'val_loss': tensor(1.1652), 'val_acc': tensor(0.8203)},\n",
       "  {'val_loss': tensor(1.2049), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.1325), 'val_acc': tensor(0.8125)},\n",
       "  {'val_loss': tensor(1.2464), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.2527), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.1724), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.2559), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.1313), 'val_acc': tensor(0.8281)},\n",
       "  {'val_loss': tensor(1.2114), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.1597), 'val_acc': tensor(0.8359)},\n",
       "  {'val_loss': tensor(1.1913), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.2208), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.2248), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.1672), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.1810), 'val_acc': tensor(0.8594)},\n",
       "  {'val_loss': tensor(1.2148), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.2406), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.2735), 'val_acc': tensor(0.7188)},\n",
       "  {'val_loss': tensor(1.1996), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.2021), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.2166), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.2225), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.2065), 'val_acc': tensor(0.8203)},\n",
       "  {'val_loss': tensor(1.1379), 'val_acc': tensor(0.8438)},\n",
       "  {'val_loss': tensor(1.1797), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.2071), 'val_acc': tensor(0.8438)},\n",
       "  {'val_loss': tensor(1.1171), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.2066), 'val_acc': tensor(0.8516)},\n",
       "  {'val_loss': tensor(1.2300), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.1671), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.2727), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.2321), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.3065), 'val_acc': tensor(0.7188)},\n",
       "  {'val_loss': tensor(1.1571), 'val_acc': tensor(0.8359)},\n",
       "  {'val_loss': tensor(1.1743), 'val_acc': tensor(0.8281)},\n",
       "  {'val_loss': tensor(1.2056), 'val_acc': tensor(0.7969)},\n",
       "  {'val_loss': tensor(1.2411), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.1587), 'val_acc': tensor(0.8438)},\n",
       "  {'val_loss': tensor(1.1879), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.1501), 'val_acc': tensor(0.8594)},\n",
       "  {'val_loss': tensor(1.2512), 'val_acc': tensor(0.8125)},\n",
       "  {'val_loss': tensor(1.1941), 'val_acc': tensor(0.8203)},\n",
       "  {'val_loss': tensor(1.1990), 'val_acc': tensor(0.7891)},\n",
       "  {'val_loss': tensor(1.2397), 'val_acc': tensor(0.7812)},\n",
       "  {'val_loss': tensor(1.2688), 'val_acc': tensor(0.7656)},\n",
       "  {'val_loss': tensor(1.1625), 'val_acc': tensor(0.8516)},\n",
       "  {'val_loss': tensor(1.1402), 'val_acc': tensor(0.7734)},\n",
       "  {'val_loss': tensor(1.2506), 'val_acc': tensor(0.8359)},\n",
       "  {'val_loss': tensor(1.1851), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.2426), 'val_acc': tensor(0.7422)},\n",
       "  {'val_loss': tensor(1.2189), 'val_acc': tensor(0.8047)},\n",
       "  {'val_loss': tensor(1.1542), 'val_acc': tensor(0.8125)},\n",
       "  {'val_loss': tensor(1.1875), 'val_acc': tensor(0.7578)},\n",
       "  {'val_loss': tensor(1.3206), 'val_acc': tensor(0.5625)}]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
